---
title: "Accurate Neural Network Pruning Requires Rethinking Sparse Optimization"
collection: publications
permalink: /publication/2024-06-sparse-opt
author: Denis Kuznedelev*, Eldar Kurtic*, <b>Eugenia Iofinova</b>*, Elias Frantar, Alexandra Peste*, Dan Alistarh
header:
  teaser: sparse_opt_thumbnail.png
excerpt: 'We show that, generally speaking, dense training settings are not optimal for sparse training for the same dataset/architecture.'
date: 2024-06-20
venue: 'TMLR'
paperurl: 'https://arxiv.org/pdf/2308.02060.pdf'
---
We show that, generally speaking, dense training settings are not optimal for sparse training for the same dataset/architecture. In particular, for computer vision, we show that extended training greatly improves the results; we explore the difficulty of finding the right recipes under sparsity.

[arXiv](https://arxiv.org/pdf/2308.02060.pdf)

